{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs \n",
    "import requests \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "links=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.ajkerbazzar.com/news/stock-market/page/'\n",
    "data_dir='C:\\Users\\Amzad\\Desktop\\ajkerbazzar_eco.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,630):   \n",
    "    \n",
    "    url = 'https://www.bd-pratidin.com/economy/'+str(i*14)\n",
    "    r = requests.get(url).text\n",
    "    soup = bs(r, 'html.parser')\n",
    "    link=soup.find_all('a')\n",
    "    link = [a['href'] for a in link]\n",
    "    for i in link:\n",
    "        if 'https://www.ajkerbazzar.com/news/' not  in i:\n",
    "            links.append(i)\n",
    "\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate links \n",
    "#links=np.load(r'C:\\Users\\Amzad\\Desktop\\news_analysis\\data\\ajkerbazzar_links(eco).npy')\n",
    "links = list(set(links)) \n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in links:\n",
    "    if 'https://www.ajkerbazzar.com/' not  in i:\n",
    "        links.remove(i)\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_link=np.array(links)    \n",
    "#save the links in a npy file\n",
    "np.save(r'C:\\Users\\Amzad\\Desktop\\news_analysis\\data\\ajkerbazzar_links(eco).npy',list_of_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\" https://www.ajkerbazzar.com/%e0%a6%a4%e0%a6%bf%e0%a6%a8-%e0%a6%ab%e0%a6%b8%e0%a6%b2%e0%a7%80-%e0%a6%9c%e0%a6%ae%e0%a6%bf%e0%a6%a4%e0%a7%87-%e0%a6%95%e0%a7%8b%e0%a6%a8-%e0%a6%a7%e0%a6%b0%e0%a6%a8%e0%a7%87%e0%a6%b0-%e0%a6%89/220554\"\n",
    "r = requests.get(test).text\n",
    "soup = bs(r, 'html.parser')\n",
    "title=soup.find('h1').text\n",
    "title\n",
    "date=soup.find('div',class_= 'col-md-8 col-xs-12 col-sm-12').text.split('|')[-1]\n",
    "date.strip().replace(',',' ').strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=soup.find('div',class_= 'entry-content').text.split('|')[0]\n",
    "new.replace('আজকের বাজার','').strip().replace('\\n','').replace('60SHARESShareTweetSubscribe','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "titles=[]\n",
    "dates=[]\n",
    "news=[]\n",
    "link=[]\n",
    "cetegory=[]\n",
    "for i in links:\n",
    "    if count==1000:\n",
    "        break\n",
    "    else:\n",
    "        try :\n",
    "            r = requests.get(i).text\n",
    "            soup = bs(r, 'html.parser')\n",
    "            title=soup.find('h1').text\n",
    "            date=soup.find('div',class_= 'col-md-8 col-xs-12 col-sm-12').text.split('|')[-1]\n",
    "            date=date.strip().replace(',',' ').strip()\n",
    "            date=soup.find('div',class_= 'col-md-8 col-xs-12 col-sm-12').text.split('|')[-1]\n",
    "            date.strip().replace(',',' ').strip()\n",
    "            new=soup.find('div',class_= 'entry-content').text.split('|')[0]\n",
    "            new=new.replace('আজকের বাজার','').strip().replace('\\n','').replace('SHARESShareTweetSubscribe','')\n",
    "            titles.append(title)\n",
    "            dates.append(date)\n",
    "            news.append(new)\n",
    "            link.append(i)\n",
    "            cetegory.append('অর্থনীতি')\n",
    "\n",
    "            # print(date)\n",
    "        except:\n",
    "            print(count)\n",
    "            print('error')\n",
    "\n",
    "    count+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Title':titles,'Link':link,'Description':news,'Date':dates,'Category':cetegory})\n",
    "df.to_csv(data_dir,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ef60fdc05b8c9d7c00029e41ed6e96799336c5489ef8cc1e2e0d70e844fa01c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
